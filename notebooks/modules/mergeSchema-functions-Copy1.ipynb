{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os, time, json\n",
    "from stat import S_ISREG, ST_CTIME, ST_MODE\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StructField, StructType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify partitions with schema changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_schema_equal(path1, path2, file_format):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - path1: Complete file path\n",
    "    - path2: Complete file path\n",
    "    \n",
    "    Output: Bool\n",
    "    Returns true if path1 and path2 have the same schema.\n",
    "    \"\"\"\n",
    "    \n",
    "    df1 = spark.read.format(file_format).load(path1).limit(0)\n",
    "    df2 = spark.read.format(file_format).load(path2).limit(0)\n",
    "    \n",
    "    return df1.schema == df2.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_schema_changes(path, file_format, partition_list = []):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - path: Entity path containing partitions\n",
    "    - file_format: File format (parquet or avro)\n",
    "    \n",
    "    Output: Dict\n",
    "    Returns a dict where each key represents a different schema with the initial path and final \n",
    "    path containing that schema version, delimiting the boundaries of that version.\n",
    "    \"\"\"\n",
    "    \n",
    "    dict = {}\n",
    "    idx = 0\n",
    "    \n",
    "    for dir in sorted([d for d in os.listdir(path) if d.find(\"=\") != -1]) if partition_list == [] else partition_list:\n",
    "        # Add new key if not exist in dict\n",
    "        if idx not in dict:\n",
    "            dict[idx] = {\"init_path\": path + \"/\" + dir, \"final_path\": path + \"/\" + dir}\n",
    "        # When schema is different, add new key to dict\n",
    "        elif not is_schema_equal(dict[idx][\"init_path\"], path + \"/\" + dir, file_format):\n",
    "            idx = idx + 1\n",
    "            dict[idx] = {\"init_path\": path + \"/\" + dir, \"final_path\": path + \"/\" + dir}\n",
    "        # When schema is equal, update final_path\n",
    "        else:\n",
    "            dict[idx][\"final_path\"] = path + \"/\" + dir      \n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List partitions filtering last_modified_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modified_partitions(dir_path, ts_filter):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - dir_path: Entity path containing partitions\n",
    "    - ts_filter: Timestamp to filter the partitions returned (filter by last modification)\n",
    "    \n",
    "    Output: List\n",
    "    Returns a list of partition directories that were modified after the ts_filter provided as parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    # All directories inside the dir_path that matches partition pattern\n",
    "    lst_dir = [d for d in os.listdir(dir_path) if d.find(\"=\") != -1]\n",
    "    \n",
    "    if lst_dir != []:\n",
    "        # Filter based on modification date\n",
    "        return sorted([(dir, round(os.stat(dir_path + \"/\" + dir).st_mtime * 1000)) for dir in lst_dir if round(os.stat(dir_path + \"/\" + dir).st_mtime * 1000) > ts_filter])\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last Read Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_read_ts(file_path):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - file_path: Complete file path of the file used to control the partition read.\n",
    "    \n",
    "    Output: Integer\n",
    "    Returns the timestamp of the last time that a partition was read.\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        # Read control file\n",
    "        with open(file_path) as inputfile:\n",
    "            ctrl_file = json.load(inputfile)\n",
    "            last_modified_ts_ctrl = ctrl_file[\"last_read_ts\"]\n",
    "    else:\n",
    "        last_modified_ts_ctrl = 0\n",
    "        \n",
    "    return last_modified_ts_ctrl\n",
    "\n",
    "def set_last_read_ts(file_path):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - file_path: Complete file path of the file used to control the partition read.\n",
    "    \n",
    "    Output: None\n",
    "    Creates or overwrite the file used to control the partition read.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get only the path without file\n",
    "    path = \"/\".join(file_path.split(\"/\")[:-1])\n",
    "    \n",
    "    # Check if path exist. If not, create\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # Save ts\n",
    "    with open(file_path, \"w+\") as outfile:\n",
    "        json.dump({\"last_read_ts\":round(time.time() * 1000)}, outfile)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Columns to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_columns_to_string(schema, parent = \"\", lvl = 0):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - schema: Dataframe schema as StructType\n",
    "    \n",
    "    Output: List\n",
    "    Returns a list of columns in the schema casting them to String to use in a selectExpr Spark function.\n",
    "    \"\"\"\n",
    "    \n",
    "    lst=[]\n",
    "    \n",
    "    for x in schema:\n",
    "        if lvl > 0 and len(parent.split(\".\")) > 0:\n",
    "            parent = \".\".join(parent.split(\".\")[0:lvl])\n",
    "        else:\n",
    "            parent = \"\"\n",
    "       \n",
    "        if isinstance(x.dataType, StructType):\n",
    "            parent = parent + \".\" + x.name\n",
    "            nested_casts = \",\".join(convert_columns_to_string(x.dataType, parent.strip(\".\"), lvl = lvl + 1))\n",
    "            lst.append(\"struct({nested_casts}) as {col}\".format(nested_casts=nested_casts, col=x.name))\n",
    "        else:\n",
    "            if parent == \"\":\n",
    "                lst.append(\"cast({col} as string) as {col}\".format(col=x.name))\n",
    "            else:\n",
    "                lst.append(\"cast({parent}.{col} as string) as {col}\".format(col=x.name, parent=parent))\n",
    "                \n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge schemas function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_schemas(dir_path, ctrl_file, file_format, mode):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    - dir_path: Entity path containing partitions.\n",
    "    - ctrl_file: Complete file path of the file used to control the partition read.\n",
    "    - file_format: File format to read (parquet or avro)\n",
    "    - mode: \"I\" for incremental or \"F\" for full\n",
    "    \n",
    "    Output: JSON RDD\n",
    "    If mode = I returns a JSON RDD containing the union of all partitions modified since the timestamp in control partition read.\n",
    "    If mode = F returns a JSON RDD containing the union of all partitions.\n",
    "    All columns are converted to String in the returning RDD.\n",
    "    \"\"\"\n",
    "    \n",
    "    idx = 0\n",
    "    last_modified_ts_ctrl = 0 if mode == \"F\" else get_last_read_ts(ctrl_file)\n",
    "    \n",
    "    # Check if there are files to process\n",
    "    if get_modified_partitions(dir_path, last_modified_ts_ctrl) != []:\n",
    "        lst_dir = [dir for dir, last_mod_ts in get_modified_partitions(dir_path, last_modified_ts_ctrl)] \n",
    "\n",
    "        try:\n",
    "            if len(identify_schema_changes(dir_path, file_format, lst_dir).keys()) > 1:\n",
    "                print(\"Different schemas identified:\")\n",
    "                print(json.dumps(identify_schema_changes(dir_path, file_format, lst_dir), indent = 4))\n",
    "            \n",
    "            print(\"\\nProcessing files:\")\n",
    "            # Read each directory and create a JSON RDD making a union of all directories\n",
    "            for dir in lst_dir:\n",
    "                print(\"idx: \" + str(idx) + \" | path: \" + dir_path + \"/\" + dir)\n",
    "\n",
    "                # Get schema\n",
    "                schema = spark.read.format(file_format).load(dir_path + \"/\" + dir).limit(0).schema\n",
    "\n",
    "                # Read file converting to string\n",
    "                df_temp = (spark.read\n",
    "                               .format(file_format)\n",
    "                               .load(dir_path + \"/\" + dir)\n",
    "                               .selectExpr(convert_columns_to_string(schema))\n",
    "                               .withColumn(dir.split(\"=\")[0], lit(dir.split(\"=\")[1]))\n",
    "                          )\n",
    "\n",
    "                # Convert to JSON to avoid error when union different schemas\n",
    "                if idx == 0:\n",
    "                    rdd_json = df_temp.toJSON()\n",
    "                else:\n",
    "                    rdd_json = rdd_json.union(df_temp.toJSON())\n",
    "\n",
    "                idx = idx + 1\n",
    "\n",
    "            # Set Timestamp in control file\n",
    "            set_last_read_ts(ctrl_file)\n",
    "\n",
    "            return rdd_json\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Unexpected error: \", e)\n",
    "                \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_schemas_incremental(dir_path, ctrl_file, file_format):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    - dir_path: Entity path containing partitions.\n",
    "    - ctrl_file: Complete file path of the file used to control the partition read.\n",
    "    - file_format: File format to read (parquet or avro)\n",
    "    \n",
    "    Output: JSON RDD\n",
    "    Returns a JSON RDD containing the union of all partitions modified since the timestamp in control partition read file.\n",
    "    All columns are converted to String in the returning RDD.\n",
    "    \"\"\"\n",
    "    \n",
    "    idx = 0\n",
    "    last_modified_ts_ctrl = get_last_read_ts(ctrl_file)\n",
    "    \n",
    "    # Check if there are new files to process\n",
    "    if get_modified_partitions(dir_path, last_modified_ts_ctrl) != []:\n",
    "        lst_dir = [dir for dir, last_mod_ts in get_modified_partitions(dir_path, last_modified_ts_ctrl)] \n",
    "\n",
    "        try:\n",
    "            if len(identify_schema_changes(dir_path, file_format, lst_dir).keys()) > 1:\n",
    "                print(\"Different schemas identified:\")\n",
    "                print(json.dumps(identify_schema_changes(dir_path, file_format, lst_dir), indent = 4))\n",
    "            \n",
    "            print(\"\\nProcessing files:\")\n",
    "            # Read each directory and create a JSON RDD making a union of all directories\n",
    "            for dir in lst_dir:\n",
    "                print(\"idx: \" + str(idx) + \" | path: \" + dir_path + \"/\" + dir)\n",
    "\n",
    "                # Get schema\n",
    "                schema = spark.read.format(file_format).load(dir_path + \"/\" + dir).limit(0).schema\n",
    "\n",
    "                # Read file converting to string\n",
    "                df_temp = (spark.read\n",
    "                               .format(file_format)\n",
    "                               .load(dir_path + \"/\" + dir)\n",
    "                               .selectExpr(convert_columns_to_string(schema))\n",
    "                               .withColumn(dir.split(\"=\")[0], lit(dir.split(\"=\")[1]))\n",
    "                          )\n",
    "\n",
    "                # Convert to JSON to avoid error when union different schemas\n",
    "                if idx == 0:\n",
    "                    rdd_json = df_temp.toJSON()\n",
    "                else:\n",
    "                    rdd_json = rdd_json.union(df_temp.toJSON())\n",
    "\n",
    "                idx = idx + 1\n",
    "\n",
    "            # Set Timestamp in control file\n",
    "            set_last_read_ts(ctrl_file)\n",
    "\n",
    "            return rdd_json\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Unexpected error: \", e)\n",
    "                \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge schemas function - version without incremental control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_schemas_full(dir_path, file_format):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    - dir_path: Entity path containing partitions.\n",
    "   \n",
    "    Output: JSON RDD\n",
    "    Returns a JSON RDD containing the union of all partitions in dir_path with columns converted to String.\n",
    "    \"\"\"\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    print(\"\\nProcessing files:\")\n",
    "    \n",
    "    # Read each directory and create a JSON RDD making a union of all directories\n",
    "    for dir in [d for d in os.listdir(dir_path) if d.find(\"=\") != -1]:\n",
    "        print(\"idx: \" + str(idx) + \" | path: \" + dir_path + \"/\" + dir)\n",
    "\n",
    "        # Get schema\n",
    "        schema = spark.read.format(file_format).load(dir_path + \"/\" + dir).limit(0).schema\n",
    "\n",
    "        # Read file converting columns to string\n",
    "        df_temp = (spark.read\n",
    "                       .format(file_format)\n",
    "                       .load(dir_path + \"/\" + dir)\n",
    "                       .selectExpr(convert_columns_to_string(schema))\n",
    "                       .withColumn(dir.split(\"=\")[0], lit(dir.split(\"=\")[1]))\n",
    "                  )\n",
    "\n",
    "        # Convert to JSON to avoid error when union different schemas\n",
    "        if idx == 0:\n",
    "            rdd_json = df_temp.toJSON()\n",
    "        else:\n",
    "            rdd_json = rdd_json.union(df_temp.toJSON())\n",
    "\n",
    "        idx = idx + 1\n",
    "\n",
    "    return rdd_json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
