{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('02_add-nested-structure')\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Set dynamic partitions to overwrite only the partition processed\n",
    "spark.conf.set('spark.sql.sources.partitionOverwriteMode', 'dynamic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mimesis import Person, Address \n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType\n",
    "\n",
    "def gen_data_add_nested_struct(data_path, partition_date, num_rows):\n",
    "    person = Person('en')\n",
    "    address = Address('en')\n",
    "    \n",
    "    # Create schema\n",
    "    schema_address = StructType(\n",
    "        [\n",
    "            StructField('address', StringType(), True),\n",
    "            StructField('city', StringType(), True),\n",
    "            StructField('country', StringType(), True),\n",
    "            StructField('state', StringType(), True),\n",
    "            StructField('postal_code', StringType(), True)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    schema_df = StructType(\n",
    "        [\n",
    "            StructField('identifier', StringType(), True),\n",
    "            StructField('first_name', StringType(), True),\n",
    "            StructField('last_name', StringType(), True),\n",
    "            StructField('occupation', StringType(), True),\n",
    "            StructField('age', IntegerType(), True),\n",
    "            StructField('address', schema_address, True),\n",
    "            StructField('date', DateType(), True)\n",
    "\n",
    "\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Generate data\n",
    "    for i in range(num_rows):\n",
    "        df_temp = spark.createDataFrame([\n",
    "            [\n",
    "                person.identifier(),\n",
    "                person.first_name(),\n",
    "                person.last_name(),\n",
    "                person.occupation(),\n",
    "                person.age(),\n",
    "                [\n",
    "                    address.address(),\n",
    "                    address.city(),\n",
    "                    address.country(),\n",
    "                    address.state(),\n",
    "                    address.postal_code()\n",
    "                ],\n",
    "                partition_date\n",
    "            ]\n",
    "        ], schema_df)\n",
    "\n",
    "        try:\n",
    "            df = df.union(df_temp)\n",
    "        except:\n",
    "            df = df_temp\n",
    "            \n",
    "    df.coalesce(1).write.partitionBy('date').mode('overwrite').parquet(data_path)\n",
    "    \n",
    "    print('Partition created: {data_path}/date={date}'.format(data_path=data_path,date=partition_date))\n",
    "    print('# Rows:',df.count())\n",
    "    print('Schema:')\n",
    "    df.printSchema()\n",
    "    print('\\n')\n",
    "    \n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
